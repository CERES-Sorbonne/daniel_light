{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62863f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e702bc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json(chemin,contenu):\n",
    "    with open(chemin,\"w\",encoding=\"utf-8\") as w :\n",
    "        w.write(json.dumps(contenu, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7946eb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ouvrir_json(chemin) :\n",
    "    with open(chemin,encoding=\"utf-8\") as f :\n",
    "        contenu = json.load(f)\n",
    "    return contenu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28adb9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "modele = spacy.load(\"fr_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e38f422",
   "metadata": {},
   "outputs": [],
   "source": [
    "#renvoie un set de stopwords\n",
    "with open(\"stopwords-fr.json\",encoding=\"utf-8\") as f:\n",
    "    set_stop = set(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a2c4cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_KW(liste):\n",
    "  out = []\n",
    "  #on enlève tous les stopwords (liste resultante)\n",
    "  #filtrage des tokens qui sont courts ou des entiers\n",
    "  liste = set(liste).difference(set_stop)\n",
    "  for x in liste:\n",
    "    numbers = re.findall(\"[0-9]\",x)\n",
    "    if x.lower() in set_stop:\n",
    "      continue\n",
    "    elif len(x)<2:\n",
    "      continue\n",
    "    elif len(re.findall(\"[A-Za-z]\", x))==0:\n",
    "      continue\n",
    "    elif len(numbers)>0:\n",
    "       if len(numbers[0])==len(x):\n",
    "         continue\n",
    "    out.append(x)\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d3c8ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(chaine):\n",
    "  chaine = re.sub(\"\\.|'|\\?|»|«|\\\"\", \" \", chaine)\n",
    "  chaine = re.sub(\" {2,}\",\" \", chaine)\n",
    "  mots = chaine.split()\n",
    "  return mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "136935ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_spacy(modele, chaine) :\n",
    "    mots = modele(chaine)\n",
    "    liste_mots = []\n",
    "    for m in mots :\n",
    "        liste_mots.append(m.text)\n",
    "    return liste_mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5de4638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_subsequence(needle, haystack):\n",
    "  return any( haystack[i:i+len(needle)] == needle\n",
    "            for i in range(len(haystack) - len(needle) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6f5fecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inter : ce qui est à la fois dans titre et body\n",
    "#last : indice\n",
    "def get_ngrams(inter, word_titre, word_body):\n",
    " \n",
    "  selected = [x for x in inter]\n",
    "  haystack=[x.lower() for x in word_body]\n",
    "  #le mot est dans le titre et dans le chapeau\n",
    "  for i, word in enumerate(word_titre):\n",
    "    if word in inter or word.lower() in inter:\n",
    "      n_gram = [word]\n",
    "      last = i\n",
    "      for j, w in enumerate(word_titre[i+1:]):\n",
    "        if w in inter:\n",
    "          last = j\n",
    "        \n",
    "        n_gram.append(w)\n",
    "      if len(n_gram)>1 and len(n_gram)<5 and last!=i:\n",
    "        needle = [x.lower() for x in n_gram]\n",
    "        if is_subsequence(needle, haystack):\n",
    "          selected.append(\"_\".join(n_gram[:last+2]))\n",
    "        \n",
    "## TODO: régler le problème des n-grammes\n",
    "##  if \"gilets\" in haystack and \"jaunes in haystack\":\n",
    "##    if \"gilets_jaunes\" not in selected:\n",
    "##      print(selected)\n",
    "##      x = haystack.index(\"gilets\")\n",
    "##      print(haystack[x:x+10])\n",
    "##      print(word_titre[word_titre.index(\"gilets\"):])\n",
    "  return selected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2136e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_KW(titre,text,spacy,chapeau_nb=0,modele=modele):\n",
    "    body = text\n",
    "    chapeau = \"\"\n",
    "    \n",
    "    #Tokenisation par défaut\n",
    "    if spacy == 0 : \n",
    "        if chapeau_nb == 0 :\n",
    "            if len(titre)<100:\n",
    "                chapeau = text[:200]\n",
    "                body = text[200:]\n",
    "        \n",
    "        if chapeau_nb != 0 :\n",
    "            phrases = modele(body)\n",
    "            body = \"\"\n",
    "            for i,p in enumerate(phrases.sents):\n",
    "                if i < chapeau_nb :\n",
    "                    chapeau+=p.text+\" \"\n",
    "                if i >= chapeau_nb :\n",
    "                    body+=p.text+\" \"\n",
    "\n",
    "        word_titre = set(tokenize(titre) + tokenize(chapeau))\n",
    "        word_body = set(tokenize(body))\n",
    "        inter = word_titre.intersection(word_body)\n",
    "        inter = filter_KW(inter)\n",
    "        inter = get_ngrams(inter, tokenize(titre), tokenize(body))\n",
    "\n",
    "    ##Tokenisation avec spacy\n",
    "    if spacy == 1 :\n",
    "        if chapeau_nb == 0 : \n",
    "            if len(titre)<100:\n",
    "                chapeau = text[:200]\n",
    "                body = text[200:]\n",
    "        if chapeau_nb != 0 :\n",
    "            phrases = modele(body)\n",
    "            body = \"\"\n",
    "            for i,p in enumerate(phrases.sents):\n",
    "                if i < chapeau_nb :\n",
    "                    chapeau+=p.text+\" \"\n",
    "                else :\n",
    "                    body+=p.text+\" \"\n",
    "        \n",
    "        word_titre = set(tokenize_spacy(modele,titre) + tokenize_spacy(modele,chapeau))\n",
    "        word_body = set(tokenize_spacy(modele,body))\n",
    "        inter = word_titre.intersection(word_body)\n",
    "        inter = filter_KW(inter)\n",
    "        inter = get_ngrams(inter, tokenize_spacy(modele,titre), tokenize_spacy(modele,body))\n",
    "      \n",
    "    return inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a9fecd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"corpus_final/corpus.json\") as f:\n",
    "  L = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56c57a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On a 3330 Documents\n",
      "Top20 : [[192, 'complot'], [159, 'France'], [138, 'monde'], [122, 'Trump'], [119, 'théories'], [119, 'politique'], [117, 'président'], [101, 'extrême'], [76, 'Donald'], [67, 'français'], [64, 'gilets'], [63, 'mouvement'], [63, 'crise'], [63, 'affaire'], [62, 'jeunes'], [60, 'pays'], [60, 'jaunes'], [60, 'enquête'], [59, 'réseaux'], [58, 'ministre']]\n",
      "Top grams : [[13, 'réseaux_sociaux'], [12, 'gilets_jaunes'], [6, 'théories_du_complot'], [5, 'autorités_constituées'], [5, 'Donald_Trump'], [4, 'théorie_du_complot'], [4, 'liberté_de_la_presse'], [4, 'fake_news'], [4, 'attaque_de_Strasbourg'], [3, 'éducation_nationale']]\n"
     ]
    }
   ],
   "source": [
    "dic = {\"index\":{}, \"index_I\":{}}\n",
    "\n",
    "for cpt, info_text in enumerate(L):\n",
    "   \n",
    "  titre = info_text[\"titre\"]\n",
    "  text = info_text[\"texte\"]\n",
    "  date = info_text[\"date\"]\n",
    "  elems_date = re.findall(\"[0-9]{1,4}\", date)\n",
    "  \n",
    "  if len(elems_date[0])==2:\n",
    "    L[cpt][\"date\"] = \"%s-%s-%s\"%(elems_date[1], elems_date[2], elems_date[0])\n",
    "  liste_KW = get_KW(titre, text,0)\n",
    "\n",
    "  \n",
    "  for KW in liste_KW:\n",
    "    if KW.lower() in dic[\"index\"]:\n",
    "      KW = KW.lower()\n",
    "    dic[\"index\"].setdefault(KW, [])\n",
    "    #pour le mot-clé, on lui associe sa ou ses position(s) (article dont il est issu)\n",
    "    dic[\"index\"][KW].append(cpt)\n",
    "    dic[\"index_I\"].setdefault(cpt, [])\n",
    "    dic[\"index_I\"][cpt].append(KW)\n",
    "\n",
    "print(\"On a %i Documents\"%(cpt+1))\n",
    "#on calcule le nombre d'articles où apparaît un mot-clé et on le range dans une liste de listes\n",
    "l_test = [[len(dic[\"index\"][kw]), kw] for kw in dic[\"index\"]]\n",
    "\n",
    "#tri des occurrences de mot-clé\n",
    "S = sorted([x for x in l_test if x[0]>1], reverse=True)\n",
    "#tri des occurrences de mot-clés qui sont des n-grammes, n>1\n",
    "S2 = sorted([x for x in l_test if \"_\" in x[1]], reverse=True)\n",
    "\n",
    "top10 = [x[1] for x in S[:10]]\n",
    "top20 = [x for x in S[:20]]\n",
    "print(\"Top20 :\",top20)\n",
    "\n",
    "topGrams = [x for x in S2[:10]]\n",
    "print(\"Top grams :\",topGrams)\n",
    "\n",
    "entetes = [\"date\", \"journal\", \"titre\"]\n",
    "out_csv = [[x for x in entetes]]\n",
    "\n",
    "for i in range(len(top10)):\n",
    "  out_csv[0].append(top10[i])\n",
    "out_csv[0].append(\"autres mots-clés\")\n",
    "out_csv[0] = \";\".join(out_csv[0])\n",
    "\n",
    "\n",
    "for ID_txt, liste_KW in dic[\"index_I\"].items(): #cpt,mots-clés\n",
    "  #recuperer texte\n",
    "  info_txt = L[ID_txt]\n",
    "  \n",
    "  ligne = [info_txt[cle] for cle in entetes]+[\"\"]*10+[[]]\n",
    "  \n",
    "  for kw in liste_KW:\n",
    "    if kw in top10:\n",
    "      ligne[top10.index(kw)+len(entetes)]=\"X\"\n",
    "    else:\n",
    "      ligne[-1].append(kw)\n",
    "  if len(ligne[2])>100:\n",
    "    ligne[2] = ligne[2][:100]+\"...\"\n",
    "  ligne[-1]=\" , \".join(ligne[-1])\n",
    "  ligne = [re.sub(\";\", \" \", x) for x in ligne]\n",
    "  out_csv.append(\";\".join(ligne))\n",
    "\n",
    "with open(\"out_complet.csv\", \"w\",encoding=\"utf-8\") as w:\n",
    "  w.write(\"\\n\".join(out_csv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57935e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------RAKE PYTHON-------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3084c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install multi-rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abb3cd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les mots-clés du corpus (Rake) sont : \n",
      "{\"c'est\": 406, 'france': 248, 'complot': 199, 'monde': 199, 'théories': 168, 'paris': 152, 'pays': 117, 'temps': 109, 'complotisme': 108, \"n'est\": 107, 'président': 102, 'attentats': 101, 'etats-unis': 100, 'médias': 99, 'français': 92, 'réseaux_sociaux': 239, 'gilets_jaunes': 132, 'fake_news': 96, 'new_york': 92, 'front_national': 67, 'donald_trump': 66, 'charlie_hebdo': 65, 'quelques_jours': 57, 'emmanuel_macron': 55, \"l'union_européenne\": 52, 'premier_ministre': 47, \"l'élection_présidentielle\": 42, 'fausses_informations': 38, 'fondation_jean-jaurès': 37, 'dix_ans': 37, 'new_york_times': 73, 'seconde_guerre_mondiale': 62, 'world_trade_center': 42, 'da_vinci_code': 36, 'site_conspiracy_watch': 34, 'première_guerre_mondiale': 23, 'recep_tayyip_erdogan': 21, 'ku_klux_klan': 19}\n"
     ]
    }
   ],
   "source": [
    "from multi_rake import Rake\n",
    "import re\n",
    "\n",
    "#renvoie les mot-clés qui apparaissent dans le plus d'articles\n",
    "def top15_corpus_rake(L,n_grams) :\n",
    "    dico_mots_cles = {}\n",
    "    r = Rake(language_code='fr',max_words=n_grams)\n",
    "    for element in L :\n",
    "        mot_cles = r.apply(element['texte'])\n",
    "        i = 0\n",
    "        #dictionnaire qui calcule le nombre d'occurrences des mots-clés pour tout le corpus\n",
    "        for mot_cle in mot_cles :\n",
    "            if mot_cle[0] not in dico_mots_cles :\n",
    "                dico_mots_cles[mot_cle[0]] = 0\n",
    "            dico_mots_cles[mot_cle[0]] += 1\n",
    "            i+=1\n",
    "            if i == 15 :\n",
    "                break\n",
    "        \n",
    "    dico_trie_mots_cles = dict(sorted(dico_mots_cles.items(), key=lambda item:item[1],reverse=True))\n",
    "    return dico_trie_mots_cles\n",
    "\n",
    "dico_kw_rake = {}\n",
    "for i in range(1,4) :\n",
    "    \n",
    "    dico_trie_mots_cles = top15_corpus_rake(L,i)\n",
    "    \n",
    "    j = 0\n",
    "    for k,v in dico_trie_mots_cles.items() :\n",
    "        #on obtient les mots clés et leur nombre d'occurrences dans les articles\n",
    "        k2 = re.sub('\\s','_',k)\n",
    "        if k2 not in dico_kw_rake :\n",
    "            dico_kw_rake[k2] = v\n",
    "        if k2 in dico_kw_rake and v > dico_kw_rake[k2] :\n",
    "            dico_kw_rake[k2] = v\n",
    "        #liste.append((k2,v))\n",
    "        j+=1\n",
    "        \n",
    "        if j == 15 :\n",
    "            #print(liste)\n",
    "            break\n",
    "        \n",
    "print(\"Les mots-clés du corpus (Rake) sont : \")\n",
    "#dico renvoie les 15 mots-clés pour chaque n-gram et pour chaque mot-clé, prend l'occurrence d'articles\n",
    "#la plus grande (avec rake, les mots-clés par article changent selon le max_words choisi)\n",
    "print(dico_kw_rake)\n",
    "#print(len(dico_kw_rake))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1efadd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calcul des mots-clés par article\n",
    "def mots_cles_article(texte) :\n",
    "    from operator import itemgetter\n",
    "    L_mots_cles = []\n",
    "\n",
    "    for n_grams in range(1,4) :\n",
    "        mots_cles = []\n",
    "        r = Rake(language_code='fr',max_words=n_grams)\n",
    "        mots_cles = r.apply(texte)\n",
    "        tri_mots_cles = sorted(mots_cles, key=itemgetter(1),reverse = True)\n",
    "        #on parcourt la liste de mots clés pour avoir une liste de tuples à la fin (et pas une liste de listes)\n",
    "        for i in range(min(len(tri_mots_cles),5)) :\n",
    "            L_mots_cles.append(re.sub('\\s','_',tri_mots_cles[i][0]))\n",
    "        \n",
    "    L_triee = sorted(L_mots_cles, key=itemgetter(1),reverse = True)\n",
    "   \n",
    "    return L_triee[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "783bcc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les mots-clés du corpus qui apparaissent à la fois avec daniel et rake sont :  {'donald_trump', 'président', 'théories', 'complot', 'français', 'monde', 'réseaux_sociaux', 'pays', 'fake_news', 'gilets_jaunes', 'france'}\n"
     ]
    }
   ],
   "source": [
    "#------------------comparaison mots clés corpus entre Daniel et rake---------------------#\n",
    "\n",
    "strong_kw = set()\n",
    "\n",
    "#on recupère les mots-clés détectés par daniel (unigrammes et n-grammes) et on les stocke dans un ensemble\n",
    "daniel_kw = top20+topGrams\n",
    "daniel_kw_set = set()\n",
    "\n",
    "#mettre tout en minuscules car rake met tout en minuscule\n",
    "for d in daniel_kw :\n",
    "    daniel_kw_set.add(d[1].lower())\n",
    "\n",
    "#on récupère les mots-clés détectés par rake\n",
    "rake_kw_set = set()\n",
    "for k,v in dico_kw_rake.items() :\n",
    "    rake_kw_set.add(k)\n",
    "\n",
    "strong_kw = daniel_kw_set.intersection(rake_kw_set)\n",
    "print(\"Les mots-clés du corpus qui apparaissent à la fois avec daniel et rake sont : \",strong_kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14d5aaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fonction qui renvoie les autres mots clés communs aux deux systèmes et qui ne sont pas des mots-clés du corpus\n",
    "def strong_other_kw(L,i,strong_kw) :\n",
    "    \n",
    "    #on récupère les mots-clés d'un article, avec les deux systèmes\n",
    "    daniel_kw = get_KW(L[i]['titre'],L[i]['texte'],0)\n",
    "    rake_kw_set = set(mots_cles_article(L[i]['texte'],0))\n",
    "    \n",
    "    #on recupère les mots-clés détectés par daniel (unigrammes et n-grammes) et on les stocke dans un ensemble\n",
    "    daniel_kw_set = set()\n",
    "    for d in daniel_kw :\n",
    "        daniel_kw_set.add(d.lower())\n",
    "    \n",
    "    #commun est l'intersection des mots-clés de daniel et rake\n",
    "    commun = set()\n",
    "    commun = daniel_kw_set.intersection(rake_kw_set)\n",
    "    #print(commun)\n",
    "    \n",
    "    #strong_other_kw est l'ensemble des mots-clés détectés par daniel et rake et qui ne sont pas des mots-clés\n",
    "    #du corpus\n",
    "    strong_other_kw = set()\n",
    "    strong_other_kw = commun.difference(strong_kw)\n",
    "    return strong_other_kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f178c6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##--------------------------------------STATISTIQUES ET AFFICHAGES----------------------------------##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eca2fd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation_kw_corpus(L,param,nb_chapeau=0,modele=modele) :\n",
    "    from time import perf_counter\n",
    "    debut = perf_counter()\n",
    "    liste = []\n",
    "    mots_cles_corpus = []\n",
    "    \n",
    "    dico_json = {}\n",
    "\n",
    "    #comparaison mots clés corpus entre Daniel et rake\n",
    "    strong_kw = set()\n",
    "\n",
    "    #on recupère les mots-clés détectés par daniel (unigrammes et n-grammes) et on les stocke dans un ensemble\n",
    "    #a faire : faire une fonction pour renvoyer ce resultat (et pas utiliser une variable)\n",
    "    daniel_kw = top20+topGrams\n",
    "    daniel_kw_set = set()\n",
    "    \n",
    "    #mettre tout en minuscules car rake met tout en minuscule\n",
    "    for d in daniel_kw :\n",
    "        daniel_kw_set.add(d[1].lower())\n",
    "\n",
    "    #on récupère les mots-clés détectés par rake\n",
    "    rake_kw_set = set()\n",
    "    for k,v in dico_kw_rake.items() :\n",
    "        rake_kw_set.add(k)\n",
    "\n",
    "    strong_kw = daniel_kw_set.intersection(rake_kw_set)\n",
    "    mots_cles_corpus = list(strong_kw)\n",
    "    \n",
    "    #defaut : pas de tokenisation spéciale, extraction des mots-clés de tout le corpus\n",
    "    if param == \"defaut\" :\n",
    "\n",
    "        dico_json[\"daniel\"] = list(daniel_kw_set)\n",
    "        dico_json[\"rake\"] = list(rake_kw_set)\n",
    "        dico_json[\"intersection_daniel_rake\"] = mots_cles_corpus\n",
    "        \n",
    "        \n",
    "        nom_fichier = \"comparaison_daniel_rake_\"+param+\"_corpus.json\"\n",
    "        \n",
    "    #rappel : daniel intersection rake (mots-clés du corpus exclus)\n",
    "    if param == \"Adefaut\" :\n",
    "        if nb_chapeau == 0 : \n",
    "            for i,element in enumerate(L) :\n",
    "                #print(i)\n",
    "                dico_json = {}\n",
    "                #on récupère les mots-clés d'un article, avec les deux systèmes\n",
    "                daniel_kw = get_KW(L[i]['titre'],L[i]['texte'],0)\n",
    "                rake_kw_set = set(mots_cles_article(L[i]['texte']))\n",
    "\n",
    "                daniel_kw_set = set()\n",
    "                for d in daniel_kw :\n",
    "                    daniel_kw_set.add(d.lower())\n",
    "\n",
    "                #commun est l'intersection des mots-clés de daniel et rake\n",
    "                commun = set()\n",
    "                commun = daniel_kw_set.intersection(rake_kw_set)\n",
    "                #print(commun)\n",
    "\n",
    "                #strong_other_kw est l'ensemble des mots-clés détectés par daniel et rake et qui ne sont pas des mots-clés\n",
    "                #du corpus\n",
    "                strong_other_kw = set()\n",
    "                strong_other_kw = commun.difference(mots_cles_corpus)\n",
    "\n",
    "                dico_json[\"titre\"] = L[i]['titre']\n",
    "                dico_json[\"date\"] = L[i]['date']\n",
    "                dico_json[\"daniel\"] = daniel_kw\n",
    "                dico_json[\"rake\"] = list(rake_kw_set)\n",
    "                dico_json[\"intersection_daniel_rake\"] = list(strong_other_kw)\n",
    "\n",
    "                liste.append(dico_json)\n",
    "                nom_fichier = \"comparaison_daniel_rake_\"+param+\"_article.json\"\n",
    "        if nb_chapeau != 0 : \n",
    "            for i,element in enumerate(L) :\n",
    "                #print(i)\n",
    "                dico_json = {}\n",
    "                #on récupère les mots-clés d'un article, avec les deux systèmes\n",
    "                daniel_kw = get_KW(L[i]['titre'],L[i]['texte'],0,nb_chapeau,modele)\n",
    "                rake_kw_set = set(mots_cles_article(L[i]['texte']))\n",
    "\n",
    "                daniel_kw_set = set()\n",
    "                for d in daniel_kw :\n",
    "                    daniel_kw_set.add(d.lower())\n",
    "\n",
    "                #commun est l'intersection des mots-clés de daniel et rake\n",
    "                commun = set()\n",
    "                commun = daniel_kw_set.intersection(rake_kw_set)\n",
    "                #print(commun)\n",
    "\n",
    "                #strong_other_kw est l'ensemble des mots-clés détectés par daniel et rake et qui ne sont pas des mots-clés\n",
    "                #du corpus\n",
    "                strong_other_kw = set()\n",
    "                strong_other_kw = commun.difference(mots_cles_corpus)\n",
    "\n",
    "                dico_json[\"titre\"] = L[i]['titre']\n",
    "                dico_json[\"date\"] = L[i]['date']\n",
    "                dico_json[\"daniel\"] = daniel_kw\n",
    "                dico_json[\"rake\"] = list(rake_kw_set)\n",
    "                dico_json[\"intersection_daniel_rake\"] = list(strong_other_kw)\n",
    "\n",
    "                liste.append(dico_json)\n",
    "                nom_fichier = \"comparaison_daniel_rake_\"+param+\"_\"+str(nb_chapeau)+\"_article.json\"\n",
    "         \n",
    "    if param == \"spacy\" :\n",
    "        if nb_chapeau == 0 : \n",
    "            for i,element in enumerate(L) :\n",
    "                #print(i)\n",
    "                dico_json = {}\n",
    "                #on récupère les mots-clés d'un article, avec les deux systèmes\n",
    "                daniel_kw = get_KW(L[i]['titre'],L[i]['texte'],1)\n",
    "                rake_kw_set = set(mots_cles_article(L[i]['texte']))\n",
    "\n",
    "                daniel_kw_set = set()\n",
    "                for d in daniel_kw :\n",
    "                    daniel_kw_set.add(d.lower())\n",
    "\n",
    "                #commun est l'intersection des mots-clés de daniel et rake\n",
    "                commun = set()\n",
    "                commun = daniel_kw_set.intersection(rake_kw_set)\n",
    "                #print(commun)\n",
    "\n",
    "                #strong_other_kw est l'ensemble des mots-clés détectés par daniel et rake et qui ne sont pas des mots-clés\n",
    "                #du corpus\n",
    "                strong_other_kw = set()\n",
    "                strong_other_kw = commun.difference(mots_cles_corpus)\n",
    "\n",
    "                dico_json[\"titre\"] = L[i]['titre']\n",
    "                dico_json[\"date\"] = L[i]['date']\n",
    "                dico_json[\"daniel\"] = daniel_kw\n",
    "                dico_json[\"rake\"] = list(rake_kw_set)\n",
    "                dico_json[\"intersection_daniel_rake\"] = list(strong_other_kw)\n",
    "\n",
    "                liste.append(dico_json)\n",
    "                nom_fichier = \"comparaison_daniel_rake_\"+param+\"_article.json\"\n",
    "                #print(i)\n",
    "        if nb_chapeau != 0 :\n",
    "            for i,element in enumerate(L) :\n",
    "                #print(i)\n",
    "                dico_json = {}\n",
    "                #on récupère les mots-clés d'un article, avec les deux systèmes\n",
    "                daniel_kw = get_KW(L[i]['titre'],L[i]['texte'],1,nb_chapeau,modele)\n",
    "                rake_kw_set = set(mots_cles_article(L[i]['texte']))\n",
    "\n",
    "                daniel_kw_set = set()\n",
    "                for d in daniel_kw :\n",
    "                    daniel_kw_set.add(d.lower())\n",
    "\n",
    "                #commun est l'intersection des mots-clés de daniel et rake\n",
    "                commun = set()\n",
    "                commun = daniel_kw_set.intersection(rake_kw_set)\n",
    "                #print(commun)\n",
    "\n",
    "                #strong_other_kw est l'ensemble des mots-clés détectés par daniel et rake et qui ne sont pas des mots-clés\n",
    "                #du corpus\n",
    "                strong_other_kw = set()\n",
    "                strong_other_kw = commun.difference(mots_cles_corpus)\n",
    "\n",
    "                dico_json[\"titre\"] = L[i]['titre']\n",
    "                dico_json[\"date\"] = L[i]['date']\n",
    "                dico_json[\"daniel\"] = daniel_kw\n",
    "                dico_json[\"rake\"] = list(rake_kw_set)\n",
    "                dico_json[\"intersection_daniel_rake\"] = list(strong_other_kw)\n",
    "\n",
    "                liste.append(dico_json)\n",
    "                nom_fichier = \"comparaison_daniel_rake_\"+param+\"_\"+str(nb_chapeau)+\"_article.json\"\n",
    "        \n",
    "\n",
    "    write_json(nom_fichier,liste)\n",
    "    fin = perf_counter()\n",
    "    duree = fin-debut\n",
    "    print(\"Durée d'execution: \",duree)\n",
    "    \n",
    "    return nom_fichier\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0620ed3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#on souhaite afficher le nombre de mots-clés par article sous forme de tableau avec la méthode Daniel\n",
    "#et l'intersection de mots clés par article avec Daniel ∩ Rake\n",
    "def resultats_daniel(longueur) :\n",
    "    \n",
    "    #on construit des dictionnaires à 3 dimensions\n",
    "    dico_resultats_daniel = {}\n",
    "    dico_resultats_intersection = {}\n",
    "    \n",
    "    \n",
    "    liste_def3 = ouvrir_json('resultats/comparaison_daniel_rake_Adefaut_3_article.json')\n",
    "    liste_def4 = ouvrir_json('resultats/comparaison_daniel_rake_Adefaut_4_article.json')\n",
    "    liste_def5 = ouvrir_json('resultats/comparaison_daniel_rake_Adefaut_5_article.json')\n",
    "    liste_spacy3 = ouvrir_json('resultats/comparaison_daniel_rake_spacy_3_article.json')\n",
    "    liste_spacy4 = ouvrir_json('resultats/comparaison_daniel_rake_spacy_4_article.json')\n",
    "    liste_spacy5 = ouvrir_json('resultats/comparaison_daniel_rake_spacy_5_article.json')\n",
    "    liste_spacy0 = ouvrir_json('resultats/comparaison_daniel_rake_spacy_article.json')\n",
    "    liste_defaut0 = ouvrir_json('resultats/comparaison_daniel_rake_Adefaut_article.json')\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(longueur) :\n",
    "        \n",
    "        #Daniel\n",
    "        dico_resultats_daniel[i+1] = {}\n",
    "        dico_resultats_daniel[i+1]['defaut'] = {}\n",
    "        dico_resultats_daniel[i+1]['spacy'] = {}\n",
    "        \n",
    "        dico_resultats_daniel[i+1]['defaut']['0'] = len(liste_defaut0[i]['daniel'])\n",
    "        dico_resultats_daniel[i+1]['defaut']['3'] = len(liste_def3[i]['daniel'])\n",
    "        dico_resultats_daniel[i+1]['defaut']['4'] = len(liste_def4[i]['daniel'])\n",
    "        dico_resultats_daniel[i+1]['defaut']['5'] = len(liste_def5[i]['daniel'])\n",
    "        dico_resultats_daniel[i+1]['spacy']['0'] = len(liste_spacy0[i]['daniel'])\n",
    "        dico_resultats_daniel[i+1]['spacy']['3'] = len(liste_spacy3[i]['daniel'])\n",
    "        dico_resultats_daniel[i+1]['spacy']['4'] = len(liste_spacy4[i]['daniel'])\n",
    "        dico_resultats_daniel[i+1]['spacy']['5'] = len(liste_spacy5[i]['daniel'])\n",
    "        \n",
    "        \n",
    "        #daniel et rake\n",
    "        dico_resultats_intersection[i+1] = {}\n",
    "        dico_resultats_intersection[i+1]['defaut'] = {}\n",
    "        dico_resultats_intersection[i+1]['spacy'] = {}\n",
    "        \n",
    "        dico_resultats_intersection[i+1]['defaut']['0'] = len(liste_defaut0[i]['intersection_daniel_rake'])\n",
    "        dico_resultats_intersection[i+1]['defaut']['3'] = len(liste_def3[i]['intersection_daniel_rake'])\n",
    "        dico_resultats_intersection[i+1]['defaut']['4'] = len(liste_def4[i]['intersection_daniel_rake'])\n",
    "        dico_resultats_intersection[i+1]['defaut']['5'] = len(liste_def5[i]['intersection_daniel_rake'])\n",
    "        dico_resultats_intersection[i+1]['spacy']['0'] = len(liste_spacy0[i]['intersection_daniel_rake'])\n",
    "        dico_resultats_intersection[i+1]['spacy']['3'] = len(liste_spacy3[i]['intersection_daniel_rake'])\n",
    "        dico_resultats_intersection[i+1]['spacy']['4'] = len(liste_spacy4[i]['intersection_daniel_rake'])\n",
    "        dico_resultats_intersection[i+1]['spacy']['5'] = len(liste_spacy5[i]['intersection_daniel_rake'])\n",
    "        \n",
    "\n",
    "    \n",
    "    return (dico_resultats_daniel, dico_resultats_intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cffacd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultats = resultats_daniel(len(L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c631e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#affichage sous forme de tableau dans une page HTML\n",
    "def affichage(dico_daniel,nom) :\n",
    "    html = \"\"\"\n",
    "    <table>\n",
    "    <tbody>\n",
    "\t<tr>\n",
    "\t<th>Article</th>\n",
    "\t<th colspan=\"4\" scope=\"colgroup\">Tokenisation defaut</th>\n",
    "\t<th colspan=\"4\" scope=\"colgroup\">Tokenisation spacy</th>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t<td></td>\n",
    "\t<th scope=\"col\">0</th>\n",
    "\t<th scope=\"col\">3</th>\n",
    "\t<th scope=\"col\">4</th>\n",
    "\t<th scope=\"col\">5</th>\n",
    "\t<th scope=\"col\">0</th>\n",
    "\t<th scope=\"col\">3</th>\n",
    "\t<th scope=\"col\">4</th>\n",
    "\t<th scope=\"col\">5</th>\n",
    "\t</tr>\n",
    "    \"\"\"\n",
    "    for k,v in dico_daniel.items() :\n",
    "        html+=\"<tr> <td>\"+str(k)+\"</td> <td>\"+str(dico_daniel[k]['defaut']['0'])+\"</td> <td>\"+str(dico_daniel[k]['defaut']['3'])+\"</td> <td>\"+str(dico_daniel[k]['defaut']['4'])+\"</td> <td>\"+str(dico_daniel[k]['defaut']['5'])+\"</td> <td>\"+str(dico_daniel[k]['spacy']['0'])+\"</td> <td>\"+str(dico_daniel[k]['spacy']['3'])+\"</td> <td>\"+str(dico_daniel[k]['spacy']['4'])+\"</td> <td>\"+str(dico_daniel[k]['spacy']['5'])+\"</tr>\"\n",
    "        html+=\"\\n\"\n",
    "\n",
    "    html+=\"</tbody> </table>\"\n",
    "    nomFichier = \"page_\"+nom+\".html\"\n",
    "    f = open(nomFichier,\"w\",encoding=\"utf-8\")\n",
    "    f.write(html)\n",
    "    #print(html)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ef9e40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# affichage(resultats[0],\"daniel\")\n",
    "# affichage(resultats[1],\"intersection\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
