{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62863f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e38f422",
   "metadata": {},
   "outputs": [],
   "source": [
    "#renvoie un set de stopwords\n",
    "with open(\"stopwords-fr.json\",encoding=\"utf-8\") as f:\n",
    "    set_stop = set(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a2c4cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_KW(liste):\n",
    "  out = []\n",
    "  #on enlève tous les stopwords (liste resultante)\n",
    "  #filtrage des tokens qui sont courts ou des entiers\n",
    "  liste = set(liste).difference(set_stop)\n",
    "  for x in liste:\n",
    "    numbers = re.findall(\"[0-9]\",x)\n",
    "    if x.lower() in set_stop:\n",
    "      continue\n",
    "    elif len(x)<2:\n",
    "      continue\n",
    "    elif len(re.findall(\"[A-Za-z]\", x))==0:\n",
    "      continue\n",
    "    elif len(numbers)>0:\n",
    "       if len(numbers[0])==len(x):\n",
    "         continue\n",
    "    out.append(x)\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d3c8ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(chaine):\n",
    "  chaine = re.sub(\"\\.|'|\\?|»|«|\\\"\", \" \", chaine)\n",
    "  chaine = re.sub(\" {2,}\",\" \", chaine)\n",
    "  mots = chaine.split()\n",
    "  return mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5de4638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_subsequence(needle, haystack):\n",
    "  return any( haystack[i:i+len(needle)] == needle\n",
    "            for i in range(len(haystack) - len(needle) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6f5fecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inter : ce qui est à la fois dans titre et body\n",
    "#last : indice\n",
    "def get_ngrams(inter, word_titre, word_body):\n",
    " \n",
    "  selected = [x for x in inter]\n",
    "  haystack=[x.lower() for x in word_body]\n",
    "  #le mot est dans le titre et dans le chapeau\n",
    "  for i, word in enumerate(word_titre):\n",
    "    if word in inter or word.lower() in inter:\n",
    "      n_gram = [word]\n",
    "      last = i\n",
    "      for j, w in enumerate(word_titre[i+1:]):\n",
    "        if w in inter:\n",
    "          last = j\n",
    "        \n",
    "        n_gram.append(w)\n",
    "      if len(n_gram)>1 and len(n_gram)<5 and last!=i:\n",
    "        needle = [x.lower() for x in n_gram]\n",
    "        if is_subsequence(needle, haystack):\n",
    "          selected.append(\"_\".join(n_gram[:last+2]))\n",
    "        \n",
    "## TODO: régler le problème des n-grammes\n",
    "##  if \"gilets\" in haystack and \"jaunes in haystack\":\n",
    "##    if \"gilets_jaunes\" not in selected:\n",
    "##      print(selected)\n",
    "##      x = haystack.index(\"gilets\")\n",
    "##      print(haystack[x:x+10])\n",
    "##      print(word_titre[word_titre.index(\"gilets\"):])\n",
    "  return selected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2136e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_KW(titre, text):\n",
    "  body = text\n",
    "  chapeau = \"\"\n",
    "\n",
    "  if len(titre)<100:\n",
    "    chapeau = text[:200]\n",
    "    body = text[200:]\n",
    "\n",
    "  word_titre = set(tokenize(titre) + tokenize(chapeau))\n",
    "  word_body = set(tokenize(body))\n",
    "  inter = word_titre.intersection(word_body)\n",
    "  inter = filter_KW(inter)\n",
    "  word_titre_2 = tokenize(titre) + tokenize(chapeau)\n",
    "  inter = get_ngrams(inter, tokenize(titre), tokenize(body))\n",
    "  return inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a9fecd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"corpus_final/corpus.json\") as f:\n",
    "  L = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56c57a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On a 3330 Documents\n",
      "Top20 : [[192, 'complot'], [159, 'France'], [138, 'monde'], [122, 'Trump'], [119, 'théories'], [119, 'politique'], [117, 'président'], [101, 'extrême'], [76, 'Donald'], [67, 'français'], [64, 'gilets'], [63, 'mouvement'], [63, 'crise'], [63, 'affaire'], [62, 'jeunes'], [60, 'pays'], [60, 'jaunes'], [60, 'enquête'], [59, 'réseaux'], [58, 'ministre']]\n",
      "Top grams : [[13, 'réseaux_sociaux'], [12, 'gilets_jaunes'], [6, 'théories_du_complot'], [5, 'autorités_constituées'], [5, 'Donald_Trump'], [4, 'théorie_du_complot'], [4, 'liberté_de_la_presse'], [4, 'fake_news'], [4, 'attaque_de_Strasbourg'], [3, 'éducation_nationale']]\n"
     ]
    }
   ],
   "source": [
    "dic = {\"index\":{}, \"index_I\":{}}\n",
    "\n",
    "for cpt, info_text in enumerate(L):\n",
    "   \n",
    "  titre = info_text[\"titre\"]\n",
    "  text = info_text[\"texte\"]\n",
    "  date = info_text[\"date\"]\n",
    "  elems_date = re.findall(\"[0-9]{1,4}\", date)\n",
    "  \n",
    "  if len(elems_date[0])==2:\n",
    "    L[cpt][\"date\"] = \"%s-%s-%s\"%(elems_date[1], elems_date[2], elems_date[0])\n",
    "  liste_KW = get_KW(titre, text)\n",
    "\n",
    "  \n",
    "  for KW in liste_KW:\n",
    "    if KW.lower() in dic[\"index\"]:\n",
    "      KW = KW.lower()\n",
    "    dic[\"index\"].setdefault(KW, [])\n",
    "    #pour le mot-clé, on lui associe sa ou ses position(s) (article dont il est issu)\n",
    "    dic[\"index\"][KW].append(cpt)\n",
    "    dic[\"index_I\"].setdefault(cpt, [])\n",
    "    dic[\"index_I\"][cpt].append(KW)\n",
    "\n",
    "print(\"On a %i Documents\"%(cpt+1))\n",
    "#on calcule le nombre d'articles où apparaît un mot-clé et on le range dans une liste de listes\n",
    "l_test = [[len(dic[\"index\"][kw]), kw] for kw in dic[\"index\"]]\n",
    "\n",
    "#tri des occurrences de mot-clé\n",
    "S = sorted([x for x in l_test if x[0]>1], reverse=True)\n",
    "#tri des occurrences de mot-clés qui sont des n-grammes, n>1\n",
    "S2 = sorted([x for x in l_test if \"_\" in x[1]], reverse=True)\n",
    "\n",
    "top10 = [x[1] for x in S[:10]]\n",
    "top20 = [x for x in S[:20]]\n",
    "print(\"Top20 :\",top20)\n",
    "\n",
    "topGrams = [x for x in S2[:10]]\n",
    "print(\"Top grams :\",topGrams)\n",
    "\n",
    "entetes = [\"date\", \"journal\", \"titre\"]\n",
    "out_csv = [[x for x in entetes]]\n",
    "\n",
    "for i in range(len(top10)):\n",
    "  out_csv[0].append(top10[i])\n",
    "out_csv[0].append(\"autres mots-clés\")\n",
    "out_csv[0] = \";\".join(out_csv[0])\n",
    "\n",
    "\n",
    "for ID_txt, liste_KW in dic[\"index_I\"].items(): #cpt,mots-clés\n",
    "  #recuperer texte\n",
    "  info_txt = L[ID_txt]\n",
    "  \n",
    "  ligne = [info_txt[cle] for cle in entetes]+[\"\"]*10+[[]]\n",
    "  \n",
    "  for kw in liste_KW:\n",
    "    if kw in top10:\n",
    "      ligne[top10.index(kw)+len(entetes)]=\"X\"\n",
    "    else:\n",
    "      ligne[-1].append(kw)\n",
    "  if len(ligne[2])>100:\n",
    "    ligne[2] = ligne[2][:100]+\"...\"\n",
    "  ligne[-1]=\" , \".join(ligne[-1])\n",
    "  ligne = [re.sub(\";\", \" \", x) for x in ligne]\n",
    "  out_csv.append(\";\".join(ligne))\n",
    "\n",
    "with open(\"out_complet.csv\", \"w\",encoding=\"utf-8\") as w:\n",
    "  w.write(\"\\n\".join(out_csv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57935e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------RAKE PYTHON-------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3084c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install multi-rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abb3cd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les mots-clés du corpus (Rake) sont : \n",
      "{\"c'est\": 406, 'france': 248, 'complot': 199, 'monde': 199, 'théories': 168, 'paris': 152, 'pays': 117, 'temps': 109, 'complotisme': 108, \"n'est\": 107, 'président': 102, 'attentats': 101, 'etats-unis': 100, 'médias': 99, 'français': 92, 'réseaux_sociaux': 239, 'gilets_jaunes': 132, 'fake_news': 96, 'new_york': 92, 'front_national': 67, 'donald_trump': 66, 'charlie_hebdo': 65, 'quelques_jours': 57, 'emmanuel_macron': 55, \"l'union_européenne\": 52, 'premier_ministre': 47, \"l'élection_présidentielle\": 42, 'fausses_informations': 38, 'fondation_jean-jaurès': 37, 'dix_ans': 37, 'new_york_times': 73, 'seconde_guerre_mondiale': 62, 'world_trade_center': 42, 'da_vinci_code': 36, 'site_conspiracy_watch': 34, 'première_guerre_mondiale': 23, 'recep_tayyip_erdogan': 21, 'ku_klux_klan': 19}\n"
     ]
    }
   ],
   "source": [
    "from multi_rake import Rake\n",
    "import re\n",
    "\n",
    "#renvoie les mot-clés qui apparaissent dans le plus d'articles\n",
    "def top15_corpus_rake(L,n_grams) :\n",
    "    dico_mots_cles = {}\n",
    "    r = Rake(language_code='fr',max_words=n_grams)\n",
    "    for element in L :\n",
    "        mot_cles = r.apply(element['texte'])\n",
    "        i = 0\n",
    "        #dictionnaire qui calcule le nombre d'occurrences des mots-clés pour tout le corpus\n",
    "        for mot_cle in mot_cles :\n",
    "            if mot_cle[0] not in dico_mots_cles :\n",
    "                dico_mots_cles[mot_cle[0]] = 0\n",
    "            dico_mots_cles[mot_cle[0]] += 1\n",
    "            i+=1\n",
    "            if i == 15 :\n",
    "                break\n",
    "        \n",
    "    dico_trie_mots_cles = dict(sorted(dico_mots_cles.items(), key=lambda item:item[1],reverse=True))\n",
    "    return dico_trie_mots_cles\n",
    "\n",
    "dico_kw_rake = {}\n",
    "for i in range(1,4) :\n",
    "    \n",
    "    dico_trie_mots_cles = top15_corpus_rake(L,i)\n",
    "    \n",
    "    j = 0\n",
    "    for k,v in dico_trie_mots_cles.items() :\n",
    "        #on obtient les mots clés et leur nombre d'occurrences dans les articles\n",
    "        k2 = re.sub('\\s','_',k)\n",
    "        if k2 not in dico_kw_rake :\n",
    "            dico_kw_rake[k2] = v\n",
    "        if k2 in dico_kw_rake and v > dico_kw_rake[k2] :\n",
    "            dico_kw_rake[k2] = v\n",
    "        #liste.append((k2,v))\n",
    "        j+=1\n",
    "        \n",
    "        if j == 15 :\n",
    "            #print(liste)\n",
    "            break\n",
    "        \n",
    "print(\"Les mots-clés du corpus (Rake) sont : \")\n",
    "#dico renvoie les 15 mots-clés pour chaque n-gram et pour chaque mot-clé, prend l'occurrence d'articles\n",
    "#la plus grande (avec rake, les mots-clés par article changent selon le max_words choisi)\n",
    "print(dico_kw_rake)\n",
    "#print(len(dico_kw_rake))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1efadd34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"susceptibles_d'y_adhérer\", \"culturel_d'une_extrême\", 'étoiles_juives_associées', 'profiter', 'rothschild', \"analyse_l'historienne_spécialiste\", 'vidéos_accusant', 'rhétorique_conspirationniste', 'destiné', 'responsables_désignés', 'semaines_autour', 'relents_souvent_antisémites', 'pandémie', \"l'épidémie\", \"d'autres_affirmant\"]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[\"qu'il\", 'rudy_reichstadt', 'succès_massif', 'créé', \"grave_erreur_d'analyse\", 'cons_pirationnisme', 'fondation_jean_jaurès', 'directeur', 'site_conspiracy_watch', 'virage_éditorial_favorable', 'membre', 'radicalités_politiques', 'faisant_soigneusement_oublier', 'a-t-il_surpris', \"l'observatoire\"]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "['gravité', 'premier_ministre', 'président_emmanuel_macron', 'confinement_partiel', 'annoncé', 'limité', 'pic_actuel', 'vingt_ans_énormément', 'temps', 'jean_castex', \"sentiment_d'absurdité_déjà\", 'pandémie', 'pays_européens', \"n'aurions_pu_imaginer\", \"d'une_durée_suffisante\"]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "['xviii_e_siècle', 'bruno_fay', 'complocratie', 'sources', 'journaliste_indépendant', 'gouvernement_mondial_occulte', 'enquête', 'financiers_masqués_dirige', 'destruction_massive', \"certitude_qu'une_poignée\", 'editions', 'bases_réelles', \"l'auteur\", \"l'existence_d'armes\", \"s'ils_mentent_là-dessus\"]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "['déjà', 'réseaux_sociaux', 'audience_relativement_importante', 'puissants_hommes_affaires', 'pu_grappiller_çà', 'conspirationnisme', \"coup_d'essai\", 'ancien_journaliste', 'anciens_prix_nobel', 'diffusion', 'pierre_barnérias', 'raison', 'pandémie', 'fausses_informations', 'mayonnaise_prend_rapidement']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "['définir', 'expressions_consacrées', 'quelques_mots', 'croyance_absolue', 'croyance_mythifiée', 'conspirationnisme', 'complotisme', 'complots_organisés_secrètement', 'nouveauté_réside_peut-être', 'logique_interne_implacable', \"lesquelles_s'imposeraient_ensuite\", 'acteurs_viseraient', 'objectifs_rigoureusement_convergents', \"c'est\", \"l'existence\"]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[\"témoigne_\\xadl'association\", 'esprits', \"sortes_d'initiatives\", \"cours_d'autodéfense_intellectuelle\", \"sociologiques_sous-tendant_l'adhésion\", 'ambitieuses', 'kit_\\xadvidéo', 'vidéos_largement_diffusées', 'théories', 'derniers_explorent', 'parfois', 'large_visée_évaluant', \"l'objet\", \"l'éducation_\\xadnationale\", \"s'est_produite_laisse\"]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "['déjà', 'juifs', \"orsqu'un\", 'tragiques_événements', 'prendre_conscience', 'courant', 'sophie_mazet', \"âges_puisqu'elles_attirent\", 'lecture_alternative', 'ras_surer_symboliquement', \"c'est\", \"l'emprise_exercée\", \"l'objet_d'infatigables_exégèses\", \"l'âge_d'internet_augure\", \"l'image_d'alain_soral\"]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "['rôle_trouble', 'résonances', 'récupération_politique', 'explique_rudy_reichstadt', 'surenchère_médiatique', 'police', \"zones_d'ombre\", 'conflit_israélo-palestinien', \"comprendre_l'arrière-fond_idéologique\", 'vieux_thèmes_anti-judéo-maçonniques', 'services', 'renseignement', 'ratés', \"n'ont_cessé_d'enfler\", \"d'un_tweet_dubitatif\"]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "['démocrate_lyndon_b', '\\xadsociété_extrêmement_mobile', 'programmes', 'professeur', 'spécialiste', 'conseil_supérieur', 'nombreux_ouvrages', 'mouvements_fascistes_significatifs', 'emmanuel_macron', 'vice-président', 'philippe_raynaud', 'christianisme_protestant_\\xadévangélique', '\\xaddispositions_morales_particulières', 'écrit', \"l'université_panthéon-assas\"]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#calcul des mots-clés par article\n",
    "def mots_cles_article(texte) :\n",
    "    from operator import itemgetter\n",
    "    L_mots_cles = []\n",
    "    #texte = L[i]['texte']\n",
    "    for n_grams in range(1,4) :\n",
    "        mots_cles = []\n",
    "        \n",
    "        r = Rake(language_code='fr',max_words=n_grams)\n",
    "        mots_cles = r.apply(texte)\n",
    "        tri_mots_cles = sorted(mots_cles, key=itemgetter(1),reverse = True)\n",
    "        #on parcourt la liste de mots clés pour avoir une liste de tuples à la fin (et pas une liste de listes)\n",
    "        for i in range(5) :\n",
    "            L_mots_cles.append(re.sub('\\s','_',tri_mots_cles[i][0]))\n",
    "   \n",
    "    L_triee = sorted(L_mots_cles, key=itemgetter(1),reverse = True)\n",
    "    return L_triee[:15]\n",
    "for i in range(10) :\n",
    "    print(mots_cles_article(L[i]['texte']))\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "783bcc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les mots-clés du corpus qui apparaissent à la fois avec daniel et rake sont :  {'pays', 'monde', 'fake_news', 'français', 'réseaux_sociaux', 'président', 'donald_trump', 'théories', 'complot', 'gilets_jaunes', 'france'}\n"
     ]
    }
   ],
   "source": [
    "#------------------comparaison mots clés corpus entre Daniel et rake---------------------#\n",
    "\n",
    "strong_kw = set()\n",
    "\n",
    "#on recupère les mots-clés détectés par daniel (unigrammes et n-grammes) et on les stocke dans un ensemble\n",
    "daniel_kw = top20+topGrams\n",
    "daniel_kw_set = set()\n",
    "\n",
    "#mettre tout en minuscules car rake met tout en minuscule\n",
    "for d in daniel_kw :\n",
    "    daniel_kw_set.add(d[1].lower())\n",
    "\n",
    "#on récupère les mots-clés détectés par rake\n",
    "rake_kw_set = set()\n",
    "for k,v in dico_kw_rake.items() :\n",
    "    rake_kw_set.add(k)\n",
    "\n",
    "strong_kw = daniel_kw_set.intersection(rake_kw_set)\n",
    "print(\"Les mots-clés du corpus qui apparaissent à la fois avec daniel et rake sont : \",strong_kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14d5aaf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le nombre de mots-clés autres détectés par Daniel et Rake est :  0\n"
     ]
    }
   ],
   "source": [
    "#Fonction qui renvoie les autres mots clés communs aux deux systèmes et qui ne sont pas des mots-clés du corpus\n",
    "def strong_other_kw(L,i,strong_kw) :\n",
    "    #on récupère les mots-clés d'un article, avec les deux systèmes\n",
    "    daniel_kw = get_KW(L[i]['titre'],L[i]['texte'])\n",
    "    rake_kw_set = set(mots_cles_article(L[i]['texte']))\n",
    "    \n",
    "    #on recupère les mots-clés détectés par daniel (unigrammes et n-grammes) et on les stocke dans un ensemble\n",
    "    daniel_kw_set = set()\n",
    "    for d in daniel_kw :\n",
    "        daniel_kw_set.add(d.lower())\n",
    "    \n",
    "    #commun est l'intersection des mots-clés de daniel et rake\n",
    "\n",
    "    commun = set()\n",
    "    commun = daniel_kw_set.intersection(rake_kw_set)\n",
    "    #print(commun)\n",
    "    \n",
    "    #strong_other_kw est l'ensemble des mots-clés détectés par daniel et rake et qui ne sont pas des mots-clés\n",
    "    #du corpus\n",
    "    strong_other_kw = set()\n",
    "    strong_other_kw = commun.difference(strong_kw)\n",
    "    return strong_other_kw\n",
    "\n",
    "\n",
    "cpt = 0\n",
    "for i in range(len(L)) :\n",
    "    if len(strong_other_kw(L,0,strong_kw)) > 0 :\n",
    "        print(strong_other_kw(L,i,strong_kw))\n",
    "        cpt+=1\n",
    "print(\"Le nombre de mots-clés autres détectés par Daniel et Rake est : \",cpt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
